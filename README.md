# Awesome-Agentic-Reasoning

A curated list of resources on agentic reasoning powered by foundation models, following the taxonomy from "From Thought to Action: A Survey on Agentic Reasoning Powered by Foundation Models" (2025). Agentic reasoning extends large language models (LLMs) into proactive agents that can plan, use tools, execute actions, incorporate feedback, store memories, and even collaborate with other agents towards complex goals. This repository organizes key papers and frameworks by category, each with links to the original publications (and code where available). Enjoy this comprehensive overview of the emerging Agentic AI landscape!
Table of Contents ğŸ“‘â€¨- Planningâ€¨- Workflow Designâ€¨- Tree Search & Algorithmic Planningâ€¨- Process Formalizationâ€¨- Decoupling & Decompositionâ€¨- External Aids & Tool-Assisted Planningâ€¨- Reward Design & Optimal Controlâ€¨- Tool Use & Integrationâ€¨- In-Context Tool Integrationâ€¨- Post-Training Tool Integrationâ€¨- Orchestration-Based Tool Useâ€¨- Retrieval-Augmented Generation (RAG)â€¨- In-Context RAGâ€¨- Post-Training RAGâ€¨- Structure-Enhanced RAGâ€¨- Executionâ€¨- Reactive Executionâ€¨- Hierarchical Executionâ€¨- End-to-End Executionâ€¨- Model-Based Executionâ€¨- Feedback & Reflectionâ€¨- Self-Reflection & Retryâ€¨- Offline Fine-Tuning & Agent Tuningâ€¨- Minimalist Retry Strategiesâ€¨- Memoryâ€¨- Passive vs Active Memoryâ€¨- Memory-Augmented Reasoningâ€¨- Reasoning-Guided Memoryâ€¨- Structured Memoryâ€¨- Multi-Agent Collaborationâ€¨- Static Collaborationâ€¨- Hierarchical Structuresâ€¨- Cascading Pipelinesâ€¨- Modular Rolesâ€¨- Dynamic Collaborationâ€¨- Adaptive Graph Topologiesâ€¨- RL-based Coordinationâ€¨- LLM-Driven Orchestrationâ€¨- Applications & Benchmarksâ€¨- Embodied Agents & Roboticsâ€¨- Scientific Discoveryâ€¨- Autonomous Research Agentsâ€¨- Medical & Clinical Agentsâ€¨- Web Navigation & Browsingâ€¨- Tool-Use & API Benchmarksâ€¨- Multi-Agent Benchmarking

ğŸ—ºï¸ Planning
How an agent formulates plans and strategies to break down tasks and decide on actions. Planning transforms a passive LLM into a problem-solving agent by decomposing goals, searching through possible steps, and structuring reasoning. Below are works categorized by their planning strategy:
Workflow Design
Structured pipelines or workflows that split tasks into stages or modules, often with an explicit plan-act loop or predefined subtasks.â€¨- Plan & Act Frameworks: â€œReWoPâ€ ã€56â€ ã€‘ (2023), â€œChaining LLMsâ€ ã€77â€ ã€‘ (2023) â€“ Multi-stage workflows decomposing tasks into perception, reasoning, and execution steps.â€¨- Human-Like SOPs: LLM+P ã€148â€ ã€‘ (2023) â€“ Incorporating optimal planning proficiency into LLMs.â€¨- State Machine Pipelines: ã€153â€ ã€‘ (2023) â€“ Transitions across different â€œX-of-Thoughtâ€ strategies.â€¨- Multimodal Workflow: PERIA ã€193â€ ã€‘ (2023) â€“ Combines perception, imagination, and action in a unified loop.â€¨- Long-Horizon Sequencing: ã€56â€ ã€‘ (2024) â€“ Explicit long-horizon plan sequences.â€¨- Code as Plans: CodePlan ã€306â€ ã€‘ (2023) â€“ Workflow for code-related planning.
Tree Search & Algorithm Simulation
Using classical search algorithms (BFS, DFS, A, MCTS, beam search) as interpretable planning scaffolds for LLMs.â€¨- Deliberative Tree-of-Thought: ã€334â€ ã€‘, ã€178â€ ã€‘, ã€162â€ ã€‘, ã€120â€ ã€‘ â€“ Simulate breadth/depth-first search through thought trees.â€¨- Heuristic Search (A): ã€287â€ ã€‘, ã€180â€ ã€‘, ã€149â€ ã€‘ â€“ Guided expansions with heuristic evaluators.â€¨- Monte Carlo Tree Search (MCTS): ã€75â€ ã€‘, ã€214â€ ã€‘, ã€265â€ ã€‘, ã€352â€ ã€‘, ã€379â€ ã€‘, ã€46â€ ã€‘, ã€34â€ ã€‘, ã€121â€ ã€‘, ã€59â€ ã€‘, ã€348â€ ã€‘, ã€244â€ ã€‘, ã€32â€ ã€‘ â€“ Controlled exploration to improve reasoning fidelity.â€¨- Beam Search Pruning: ã€321â€ ã€‘, ã€66â€ ã€‘, ã€217â€ ã€‘ â€“ Efficiently narrow down reasoning branches.â€¨- Learned Search Policies: ã€62â€ ã€‘ (2023) â€“ Train LLMs to navigate search trees.â€¨- Hybrid Fast & Slow Planning:** ã€240â€ ã€‘ (2023) â€“ Mix reactive (fast) and deliberative (slow) planning modes.
Process Formalization
Formal representations of plans using code, logic, or planning languages for structure and interpretability.â€¨- Symbolic Logic & PDDL: ã€68â€ ã€‘, ã€176â€ ã€‘, ã€117â€ ã€‘, ã€78â€ ã€‘, ã€285â€ ã€‘ â€“ Generate plans as logical expressions or PDDL programs for classical planners.â€¨- Programmatic Plans: CodePlan ã€306â€ ã€‘ (2023) â€“ Synthesize code that encodes the plan (also listed above).â€¨- Bridging LLMs & Planners: ã€176â€ ã€‘, ã€117â€ ã€‘ â€“ Use LLMs to produce structured plans consumable by external solvers.â€¨- Explainability: Formal plans improve transparency and robustness of agent behaviors.
Decoupling & Decomposition
Modularizing the reasoning process into sub-components (e.g. separate modules for observation, memory, planning).â€¨- Observation vs Reasoning: ReWOO ã€324â€ ã€‘ (2023) â€“ Splits world observation and reasoning for efficiency.â€¨- Hierarchical Abstractions: ã€50â€ ã€‘, ã€161â€ ã€‘, ã€129â€ ã€‘, ã€293â€ ã€‘, ã€285â€ ã€‘, ã€114â€ ã€‘ â€“ Divide reasoning into reusable sub-reasoners or hierarchies.â€¨- Hyper-Tree Decomposition: ã€71â€ ã€‘ (2023) â€“ Hierarchical CoT via hypertrees of thought.â€¨- World Abstraction: ã€141â€ ã€‘ (2023) â€“ Use symbolic predicates to simplify planning space.â€¨- Latent State Decomposition: ã€337â€ ã€‘, ã€121â€ ã€‘ â€“ Factor reasoning through latent variables or state spaces.â€¨- Neuro-Symbolic Hybrids: Common in long-horizon and multi-agent planning (e.g. ã€384â€ ã€‘, ã€189â€ ã€‘).
External Aids & Tool-Assisted Planning
Integrating external knowledge or tools to help plan complex tasks.â€¨- Knowledge-Augmented Planning: ã€14â€ ã€‘, ã€27â€ ã€‘, ã€40â€ ã€‘, ã€181â€ ã€‘, ã€387â€ ã€‘ â€“ Inject structured knowledge (graphs, knowledge bases, etc.) into planning prompts.â€¨- Retrieval-Augmented Planning: ã€347â€ ã€‘, ã€135â€ ã€‘, ã€405â€ ã€‘ â€“ Fetch relevant information (documents, code, etc.) mid-plan to inform next steps.â€¨- World Models: ã€75â€ ã€‘, ã€68â€ ã€‘, ã€219â€ ã€‘, ã€395â€ ã€‘, ã€390â€ ã€‘, ã€63â€ ã€‘, ã€160â€ ã€‘, ã€289â€ ã€‘ â€“ Use learned simulators or environment models for model-based planning.â€¨- API/Tool-Driven Planning: HuggingGPT ã€249â€ ã€‘ (2023) â€“ An LLM controller delegates subtasks to expert models (tools) [1][2]. ToolPlanner ã€154â€ ã€‘ (2023) â€“ Plan with a predefined API toolset. RetroInText ã€114â€ ã€‘ â€“ Plan by retrieving and executing tool-usage examples.
Reward Design & Optimal Control
Formulating planning as an optimization problem with rewards or cost functions.â€¨- Utility Learning: Reflexion ã€255â€ ã€‘ (2023), Reflect-then-Plan ã€100â€ ã€‘ (2023), Rational Decision Making ã€339â€ ã€‘ (2022) â€“ Use learned reward models or intrinsic utility to guide plan selection.â€¨- Reward Modeling: ã€31â€ ã€‘ (2025) â€“ Train models to estimate task-specific rewards for better planning alignment.â€¨- Reward Shaping: ã€169â€ ã€‘ (2023) â€“ Handcraft or learn reward functions to encourage desired plans.â€¨- Optimal Control Techniques: ã€171â€ ã€‘, ã€194â€ ã€‘, ã€179â€ ã€‘ â€“ Apply control theory to find optimal action sequences.â€¨- Diffusion Trajectory Optimization: ã€320â€ ã€‘, ã€318â€ ã€‘, ã€248â€ ã€‘ â€“ Sample and optimize plan trajectories using diffusion models.â€¨- Offline Reinforcement Learning: ã€121â€ ã€‘, ã€238â€ ã€‘, ã€293â€ ã€‘ â€“ Improve planners using offline RL with pretrained models or cost predictors.
ğŸ”§ Tool Use & Integration
How agents invoke external tools/APIs or augment themselves with new capabilities. Tool use allows an agent to overcome its built-in limitations by calling calculators, search engines, APIs, etc. Key approaches:
In-Context Tool Integration
Prompt-based techniques to teach a frozen LLM to use tools within its chain-of-thought, without additional training.â€¨- Interleaving Reasoning & Acting: ReAct ã€335â€ ã€‘ (ICLR 2023) â€“ The seminal Reason+Act paradigm where the model reasons step-by-step and decides on tool actions in an interwoven loop[3][4]. ReAct enables dynamic tool use through CoT augmentation. Codeâ€¨- Few-Shot Tool Demos: ART ã€203â€ ã€‘ (2023) â€“ Retrieves a successful multi-step tool-use example from a library and uses it as a prompt for a new task[3].â€¨- Context Optimization: Provide clear tool documentation and usage examples to improve zero-shot tool adoption ã€87â€ ã€‘, ã€355â€ ã€‘ (2023). GEAR ã€167â€ ã€‘ â€“ Offload tool selection to a smaller model to guide a larger LLM[5]. AVATAR ã€312â€ ã€‘ â€“ Prompt the agent to perform contrastive reasoning before acting[6].â€¨- Tool Creation by Prompting: LATM ã€20â€ ã€‘ (2023) â€“ One LLM acts as a Tool Maker (writing new code tools) and another as a Tool User, so the agent can generate new tools on the fly[4]. CRAFT ã€354â€ ã€‘, CREATOR ã€215â€ ã€‘ â€“ Prompt frameworks to create domain-specific tools when needed. ToolMaker ã€314â€ ã€‘ â€“ Even converts entire code repositories into callable tools[7].
Post-Training Tool Integration
Fine-tuning or RL methods to imbue the model with tool-use skills beyond what prompting alone can do.â€¨- Supervised Fine-Tuning (SFT): Early works ã€335â€ ã€‘, ã€243â€ ã€‘, ã€220â€ ã€‘, ã€272â€ ã€‘, ã€165â€ ã€‘, ã€263â€ ã€‘, ã€212â€ ã€‘, ã€342â€ ã€‘, ã€252â€ ã€‘ â€“ Train on curated <reasoning, tool use> transcripts to bootstrap tool-use ability[8]. Toolformer ã€243â€ ã€‘ (2023) â€“ Self-supervised approach where an LLM inserts and validates API calls in text, then fine-tunes on the filtered data[9]. ToolLLM ã€221â€ ã€‘ â€“ Supervised on 16k real APIs for broad tool invocation competence[10]. ToolAlpaca ã€273â€ ã€‘ â€“ Automatically generates a multi-turn tool-use dataset via simulation and fine-tunes smaller LMs[11]. (Note: SFT alone can overfit to training patterns, leading to brittle tool use[12].)â€¨- Reinforcement Learning (RL): Subsequent works use RL to master tool use beyond imitation ã€393â€ ã€‘, ã€216â€ ã€‘, ã€304â€ ã€‘, ã€28â€ ã€‘, ã€377â€ ã€‘, ã€105â€ ã€‘, ã€49â€ ã€‘ â€“ reward models encourage effective, not just imitative, tool use[13][14]. e.g. SWE-RL ã€304â€ ã€‘ â€“ RL fine-tunes a code editor agent, ReSearch ã€28â€ ã€‘ â€“ RL for adaptive web search queries, ToolRL ã€216â€ ã€‘ â€“ introduces reward functions for multi-tool learning. RL-trained tool policies show better robustness and transfer to new tasks[14].
Orchestration-Based Tool Use
When solving complex tasks requires multiple tools in coordination, not just one tool call at a time. These approaches plan and manage sequences of tool API calls:â€¨- Multi-Tool Planning: Early frameworks ã€250â€ ã€‘, ã€140â€ ã€‘, ã€76â€ ã€‘ â€“ Strategies to chain tools together for multi-step tasks.â€¨- Centralized Orchestrator: HuggingGPT (JARVIS) ã€249â€ ã€‘ (2023) â€“ Uses an LLM as a controller to route subtasks to specialized models (tools) on the fly[15]. Codeâ€¨- Workflow Automation: TaskMatrix.AI ã€140â€ ã€‘ â€“ Connects an LLM to millions of APIs by generating task plans and matching subtasks to appropriate tools[16].â€¨- Tool Chaining Frameworks: ToolkenGPT ã€76â€ ã€‘ (2023), VOLTA ã€173â€ ã€‘ (2023) â€“ Systems for invoking multiple tools in sequences (e.g., ToolkenGPT tokenizes tool calls).â€¨- Tool-Expert Networks: ã€376â€ ã€‘, ã€378â€ ã€‘, ã€382â€ ã€‘, ã€400â€ ã€‘, ã€407â€ ã€‘ â€“ Learn or plan the optimal ordering of tool invocations for complex jobs.
ğŸ” Retrieval-Augmented Generation (RAG)
Enabling agents to retrieve external information (documents, facts, code) as part of their reasoning. RAG turns an LLM into an open-book system that can query a knowledge source mid-thought.
In-Context RAG
Embed retrieval steps directly into the agentâ€™s reasoning prompt.â€¨- Self-Ask with Search: ã€33â€ ã€‘, ã€96â€ ã€‘, ã€282â€ ã€‘ â€“ Agents pose sub-questions and use a search API, then incorporate results into reasoning.â€¨- Agentic RAG Prompting: Intermix Chain-of-Thought with retrieval actions. ReAct + Search ã€335â€ ã€‘ â€“ One can combine ReAct reasoning with web search tools.â€¨- Self-RAG: ã€10â€ ã€‘ (Asai et al. 2023) â€“ The agent retrieves, generates, and critiques its own answers in an in-context loop (self-reflective RAG).
Post-Training RAG
Train the model to become retrieval-aware via fine-tuning or specialized architecture.â€¨- Retriever-Reader Training: ã€91â€ ã€‘, ã€187â€ ã€‘ (WebGPT), ã€333â€ ã€‘ â€“ Jointly train a retriever and generator so the LLM learns when and what to retrieve.â€¨- RAG Fine-Tuning: ã€243â€ ã€‘ (Toolformer), ã€268â€ ã€‘, ã€370â€ ã€‘, ã€398â€ ã€‘ â€“ Fine-tune LLMs on multi-hop QA with retrieval steps, so they learn to issue search queries as needed.â€¨- Decision Transformer for RAG: ã€134â€ ã€‘, ã€145â€ ã€‘ â€“ Approaches that treat the retrieval sequence as a decision-making problem and optimize it.
Structure-Enhanced RAG
Augment retrieval with structured knowledge or systems for better reasoning.â€¨- Knowledge Graph RAG: ã€123â€ ã€‘ â€“ Use a knowledge graph as a source and have the agent reason with graph traversal + text retrieval.â€¨- Tool-Integrated RAG: ã€251â€ ã€‘ â€“ Agents that both retrieve and use tools (e.g., a calculator or code executor) in one pipeline.â€¨- Symbolic RAG: ã€367â€ ã€‘ â€“ Represent retrieved facts in a structured form (tables, graphs) that the agent can directly manipulate or reason over, improving consistency.
(Agentic RAG vs Traditional RAG: Traditional RAG pipelines like ã€128â€ ã€‘, ã€94â€ ã€‘ run a fixed retrieve-then-read process. Agentic RAG systems instead allow iterative, goal-directed retrieval with the agent deciding when/how to query as part of its reasoning[17].)*
ğŸ® Execution Reasoning
How an agent bridges high-level plans to low-level actions in an environment (e.g. web browser, code interpreter, robotics simulator). Execution reasoning ensures the agentâ€™s plans turn into concrete results.
Reactive Style
Tightly loop reasoning with acting step-by-step, reacting to environment feedback in real time. Ideal for dynamic or uncertain situations.â€¨- Thinking while Acting: WebGPT ã€187â€ ã€‘ â€“ Browser agent that iteratively reads the webpage state, reasons, clicks or types, and repeats.â€¨- ReAct-Based Controllers: ã€335â€ ã€‘, ã€45â€ ã€‘ â€“ Use the ReAct loop for continuous observeâ†’thinkâ†’act cycles[18].â€¨- Adaptive In-Context Examples: ã€168â€ ã€‘ (2023) â€“ Dynamically adjust few-shot examples based on past outcomes to improve robustness.â€¨- Code Debugging Loops: ã€296â€ ã€‘, ã€207â€ ã€‘, ã€255â€ ã€‘, ã€72â€ ã€‘ â€“ Agents write code, execute it, observe errors or outputs, and refine code in successive steps (reactive code refinement).â€¨- Collaborative Reactivity: ã€231â€ ã€‘, ã€80â€ ã€‘ â€“ Multi-agent teams providing feedback to each other in a reactive loop (e.g. one writes code, another reviews in real-time).â€¨- Self-Refinement Loops: ã€235â€ ã€‘, ã€5â€ ã€‘, ã€103â€ ã€‘ â€“ Agent iteratively improves its solution by self-evaluation (blend of execution and feedback).â€¨- Robotics Control: ã€259â€ ã€‘ â€“ Use an LLM as a safety monitor that interrupts a robotâ€™s actions if anomalies are detected (reactive safety override)[19]. ã€246â€ ã€‘ â€“ Split a robot task into subgoals and execute with real-time vision feedback (landmark-based adjustments)[20].
Hierarchical Style
Separate the decision process into high-level planning and low-level execution modules. Improves modularity and long-horizon coherence.â€¨- Planner-Executor Pattern: ã€1â€ ã€‘, ã€70â€ ã€‘ â€“ LLM acts as a high-level planner (in natural language) and a subordinate module executes those steps in the environment[21].â€¨- Memory-Augmented Control: ã€3â€ ã€‘ â€“ Add a memory controller to store intermediate results, allowing more coordinated execution across steps[22].â€¨- Hierarchical Task Trees: ã€71â€ ã€‘, ã€195â€ ã€‘, ã€25â€ ã€‘ â€“ Recursively break tasks into sub-tasks (tree of goals), solve leaves, and propagate results upward[23].â€¨- Search within Plans: ã€90â€ ã€‘, ã€8â€ ã€‘ â€“ Integrate Monte Carlo Tree Search or hierarchical RL inside the planning pipeline for strategic decision-making[24].â€¨- Behavior Trees for Robotics: ã€170â€ ã€‘, ã€21â€ ã€‘, ã€98â€ ã€‘ â€“ Generate hierarchical behavior trees from high-level instructions.â€¨- LLM + Low-Level Controller: ã€4â€ ã€‘, ã€93â€ ã€‘ â€“ LLM produces a high-level plan (in language or pseudo-code), which is executed by classical robot controllers or code interpreters[25].
End-to-End Style
Bypass explicit planning and learn a direct mapping from observations to actions with the LLM/VLM. Sacrifices some transparency for simplicity.â€¨- Visual & GUI Agents: ã€81â€ ã€‘, ã€144â€ ã€‘ â€“ Jointly process text instructions and GUI or image observations to directly output actions (e.g. clicking UI elements).â€¨- Web Navigation: WebGPT ã€187â€ ã€‘ â€“ Uses an LLM to directly decide browser actions from the DOM/text context, without a separate planner.â€¨- Embodied End-to-End: ã€18â€ ã€‘, ã€403â€ ã€‘, ã€52â€ ã€‘ â€“ Vision-Language Models that map camera images + text goals to robot actions in one shot.â€¨(Note: End-to-end agents can be harder to interpret or debug when mistakes occur, due to lack of intermediate explanations[26].)
Model-Based Style
Leverage an internal world model to simulate outcomes of candidate actions before executing them, enabling foresight. Especially useful for long-term or risky tasks.â€¨- Web Environment Simulation: ã€67â€ ã€‘, ã€57â€ ã€‘ â€“ LLM â€œimaginesâ€ what would happen for each possible web action (like a mental model of the website) and chooses the safest/best path offline[27].â€¨- Simulated Coding Environment: ã€304â€ ã€‘, ã€202â€ ã€‘ â€“ Train a model to simulate code execution results and long-term effects of edits, so the agent can plan code changes with awareness of future consequences[28].â€¨- Explicit Program State Tracking: ã€190â€ ã€‘, ã€9â€ ã€‘ â€“ The agent maintains a structured model of the programâ€™s state or dependencies to foresee how one change will affect others[29].â€¨- Robotics MPC with LLMs: ã€35â€ ã€‘ â€“ Learn a world model for a robotic task and use model-predictive control (MPC) to plan actions that optimize a cost over a future horizon[30]. ã€97â€ ã€‘ â€“ Use an LLM to translate high-level constraints into a cost function, then solve with MPC for safe long-horizon plans[31].â€¨(Model-based agents can anticipate long-term outcomes and avoid irreversible mistakes, at the cost of training a reliable world model.)
ğŸ”„ Feedback and Retry
How an agent incorporates feedback, self-reflection, or human signals to improve its performance over multiple attempts. Feedback mechanisms let agents correct their mistakes and learn from experience even at inference time.
Self-Reflection & Retry
Use the agentâ€™s own critique of its reasoning or output as feedback for improvement.â€¨- Reflexion ã€255â€ ã€‘ (2023) â€“ Introduces a self-reflective loop: the agent generates an initial solution, critiques it in natural language, and then tries again with those self-generated insights[32][33]. This yields rapid quality gains without model retraining. Codeâ€¨- Self-Refine ã€174â€ ã€‘ (NeurIPS 2023) â€“ The agent repeatedly critiques and revises its answer, akin to an internal editor, until it converges or improves substantially.â€¨- Multi-Agent Reflection: Some works extend reflection to multiple agents (e.g., a â€œcriticâ€ agent provides feedback to a â€œworkerâ€ agent). This is under exploration for coordinated self-improvement.
Offline Fine-Tuning & Agent Tuning
Incorporate feedback signals by training the model on trajectories or instructions that emphasize successful retries.â€¨- AgentTuning ã€357â€ ã€‘ (2023) â€“ Fine-tune on a dataset of agent trajectories (AgentInstruct) to imbue the model with a general capability to recover from mistakes[34]. This improves robustness across many tasks but requires a large offline dataset of feedback and corrections.â€¨- Learning from Failures: ã€5â€ ã€‘, ã€51â€ ã€‘ â€“ Augment training data with examples of initial failures plus corrections (either generated or human-provided)[35]. The model thus learns to avoid or fix errors if it encounters similar situations. AgentTuning-style approaches bake in reflection during training (trading real-time flexibility for stability).
Minimalist Retry Loops
Simplified trial-and-error strategies without explicit reflective reasoning.â€¨- Implicit Feedback via Repetition: ã€42â€ ã€‘ (ReZero, 2023) â€“ The agent simply retries the task from scratch multiple times, relying on chance or slight prompt randomness to eventually succeed on tasks with binary success/failure feedback[36]. No memory of past tries is retained (aside from what the environment/state reveals).â€¨- Success-Signal-Driven: ã€211â€ ã€‘ (Potamitis et al. 2023) â€“ Similarly, use only a success/failure signal to loop until success or max attempts, without analyzing why it failed.â€¨(These approaches are easy to implement and require no extra model capacity, but they might need many attempts and donâ€™t learn from mistakes beyond the single session[37].)
ğŸ—ƒï¸ Memory
How agents store, retrieve, and leverage long-term memories of past interactions or knowledge. Memory provides an agent with continuity across time beyond the fixed context window.
	â€¢	Internal vs External Memory: Agents accumulate internal memories during a session (e.g. summaries of previous events) and can access external knowledge bases via search or databases[38][39]. An agentâ€™s total memory is the union of both.
	â€¢	Modern agents treat memory as an active component of reasoning â€“ not just a transcript buffer, but something to query, update, and reason about.
Passive vs Active Memory
Early LLM agents used memory as a passive log (append all past dialogue), whereas newer approaches enable active memory management: deciding what to store, when to recall it, and how to use it for planning[40][41].â€¨- Memory Search (Retrieval): Agents often embed and retrieve relevant past info (similar to RAG). Tools like LangChain and LlamaIndex ã€150â€ ã€‘ (2022) make it easy to fetch context from vector stores.â€¨- Cache Models: MemGPT ã€201â€ ã€‘ (2023) â€“ Treats memory like an OS cache, retrieving similar past states as context[42].â€¨- Production-Scale Memory: Mem0 (Memory Operative) ã€37â€ ã€‘ (2025) â€“ Scalable long-term memory architecture for deployed AI agents.
Memory-Augmented Reasoning
Using memory to enhance reasoning quality (especially via retrospective reflection or plan adjustment).â€¨- Scratchpads & Journals: Agents keep an episodic memory of important events and intermediate results, which they can consult to avoid redundant thinking.â€¨- Retrospective Reflection: An agent might summarize what it has done so far and analyze it (storing a â€œlesson learnedâ€ entry in memory) before moving on. This can prevent repeating errors and help with very long tasks.â€¨- Memory-Driven Planning: Some works allow the memory to inform future decisions: e.g. if a past strategy worked poorly (stored in memory), the agent tries a different approach next time.
Reasoning-Guided Memory
Conversely, the agentâ€™s reasoning process can control the memory: deciding what to write to memory, when to read from it, or even when to forget.â€¨- Policy for Memory Usage: Self-Goal (SELFGOAL) ã€331â€ ã€‘ (2024) â€“ Agents that determine which high-level goals are already solved (thus can be remembered as done) to focus on new objectives.â€¨- Selective Recording: Agents might only commit certain events to memory (e.g. â€œsignificant outcome X happenedâ€ but not trivial details). This requires a reasoning policy for memory writing.â€¨- Memory Evolution: Some research explores dynamically updating memory entries (e.g. merging similar memories, pruning contradictory ones) under the agentâ€™s control.
Structured Memory
Moving beyond plain text, memories can be stored in structured forms (graphs, databases, key-value stores) to support more complex reasoning.â€¨- Knowledge Graph Memory: Agents represent their accumulated knowledge as a graph of entities/relations, enabling symbolic reasoning queries over memory (e.g. path finding for reasoning).â€¨- Workflow Logs: Agent Workflow Memory ã€301â€ ã€‘ (2024) â€“ Stores an agentâ€™s past actions and outcomes in a structured workflow log, so it can analyze what sequence of steps were effective before.â€¨- MemoryBank ã€388â€ ã€‘ (2023) â€“ A memory component for LLMs that actively archives and retrieves past dialogues to enhance long-horizon coherence.â€¨- Synergistic Memory+Reasoning: MEMâ€“1 ã€394â€ ã€‘ (2025) â€“ Proposes learning how to synergize a dedicated memory module with the reasoning module for more efficient long-horizon task solving.
(Challenges: Designing memory systems that scale, avoid distraction by irrelevant memories, and remain up-to-date is an open problem. Agents also need mechanisms to forget or compress memory to prevent overload[43][44].)*
ğŸ¤ Multi-Agent Collaboration
How multiple agents coordinate and communicate to solve tasks together. Multi-agent systems can divide complex tasks into roles (specialization) or provide redundancy and feedback to each other.
Multi-agent reasoning can be structured (static) or adaptive (dynamic)[45][46]:
Static Collaboration
A fixed coordination pattern is predetermined (agent roles, interaction order do not change during runtime). This yields predictable workflows but less adaptability[47][48].
Hierarchical Structures: A central manager agent delegates tasks to subordinate worker agents in a top-down tree.â€¨- AgentOrchestra ã€371â€ ã€‘ (2025) â€“ A â€œconductorâ€ LLM orchestrates specialized tool agents in a strict hierarchy.â€¨- MetaGPT ã€85â€ ã€‘ (2023) â€“ Assigns software engineering roles (PM, Architect, Coder, Tester) to different LLM agents with one as leader.â€¨- SurgRAW ã€164â€ ã€‘ (2023) â€“ Hierarchical agents for surgical assistance (vision, language, etc.), coordinated via a central planner.â€¨(Pro: clear division of labor; Con: limited peer-to-peer interaction or adaptability.)
Cascading Pipelines: Agents are arranged in a fixed sequence (often a directed acyclic graph); each agentâ€™s output feeds into the next.â€¨- Collab-RAG ã€327â€ ã€‘ (2023) â€“ Small LLM does query decomposition â†’ large LLM does answering with retrieval, in a 2-stage pipeline.â€¨- MA-RAG ã€191â€ ã€‘ (2023) â€“ Four agents in sequence: Planner, Step Decomposer, Retriever, QA Answerer[49]. Rigid order, no revisiting earlier steps.â€¨- Chain-of-Agents ã€374â€ ã€‘ (2023) â€“ A chain processes a long context piecewise: multiple â€œreaderâ€ agents each summarize a part, then a final agent composes the answers.â€¨- AutoAgents ã€23â€ ã€‘ (2023) â€“ Hard-coded sequence of specialist agents invoked one after another as defined by a prompt script.â€¨(Pro: easy to understand and debug; Con: errors compound down the chain, no opportunity for mid-course correction[50][51].)
Modular Role-Decomposed Architectures: Agents specialize by function (e.g. a Retriever, a Reasoner, a Executor) but are not forced into strict hierarchy or order; they may operate in parallel or on-demand.â€¨- RAG-KG-IL ã€349â€ ã€‘ (2024) â€“ Fixed roles for retrieval, knowledge graph building, and reasoning; each module-agent loops until criteria met[52].â€¨- SMoA ã€131â€ ã€‘ (2023) â€“ Multiple agents propose different answers in parallel (to a given question) without direct interaction; a separate mechanism may choose among them.â€¨- M-DocAgent ã€73â€ ã€‘ (2023) â€“ Splits a multimodal document QA task between a vision expert agent and a text expert agent working together.
Dynamic Collaboration
Agents interact through an adaptive communication topology that can change during reasoning (who talks to whom, when). This allows flexibility and emergent coordination at the cost of complexity[53][54].
Adaptive Graph Topologies: Model the multi-agent team as a graph and learn or evolve this graph during the task.â€¨- Graph Generation: Learn from scratch which agents should communicate. GComm (GommFormer) ã€88â€ ã€‘ â€“ Generates a communication graph via a transformer that outputs adjacency matrices (continuous relaxation)[55]. Variational Architect ã€365â€ ã€‘ â€“ Uses a variational autoencoder to produce a context-specific communication graph (starting from a fully connected â€œvirtualâ€ graph)[56]. MCGD ã€359â€ ã€‘ â€“ Employ diffusion models to generate sparse coordination graphs with diverse structures.â€¨- Graph Pruning: Start with an all-to-all network and prune unnecessary links/nodes for efficiency. AgentPrune ã€364â€ ã€‘ â€“ Formulate communication pruning as selecting a subgraph that maximizes performance under token/message cost constraints[57]. AGP ã€130â€ ã€‘ â€“ Soft-prune edges and hard-prune agents via dual criteria, yielding a per-query optimized topology.â€¨- Topology Search: Use search algorithms to find an effective multi-agent communication structure. AFlow ã€368â€ ã€‘ â€“ Monte Carlo Tree Search over possible workflow graphs (from a library of agent â€œoperatorsâ€) to discover good arrangements[58]. MASS ã€391â€ ã€‘ â€“ Define a set of graph motifs (e.g. debate, tool-use patterns) and search within that reduced space for the best topology[59]. MaAS ã€366â€ ã€‘ â€“ Train a controller to sample a communication subgraph (like a supernet of agent roles) conditioned on the task, instead of a fixed graph.
RL-based Coordination: Treat the multi-agent system as a Markov game or Dec-POMDP and use reinforcement learning signals to improve coordination policies.â€¨- Decentralized Advantage Actor-Critic: MAGRPO ã€152â€ ã€‘ â€“ Each agent uses a group reward signal (no centralized critic) to learn dialog policies that maximize a shared objective, enabling fully decentralized training/execution[60]. MHGPO ã€24â€ ã€‘ â€“ Extends to heterogeneous roles (different reward structures per role) but still optimizes a joint group performance[61]. COPY ã€172â€ ã€‘ â€“ Two agents co-train with shared rewards and a KL penalty to encourage them to learn complementary behaviors (pioneer & observer roles).â€¨- LLM-Generated Rewards: LGC-MARL ã€101â€ ã€‘ â€“ Uses an LLM as a reward function generator, translating a task description into a shaped reward vector for the multi-agent system[62]. LAMARL ã€396â€ ã€‘ â€“ LLM suggests an initial policy and reward function, which are then fine-tuned via RL. MAPoRL ã€204â€ ã€‘ â€“ Combines LLM-based verification scores (for current and predicted future steps) into a reward for multi-agent PPO.â€¨- Human Feedback in MARL: M3HF ã€300â€ ã€‘ â€“ Gather human preferences on multi-agent outcomes (ratings, pairwise comparisons, explanations) and convert into reward shaping signals for training[63]. O-MAPL ã€19â€ ã€‘ â€“ Use offline human preference data to directly train agent Q-values (avoiding needing a separate reward model).
LLM-Driven Orchestration: Use an LLM itself to plan the multi-agent interactions â€“ effectively one agent takes on a manager/orchestrator role for the whole team.â€¨- AutoML-Agent ã€280â€ ã€‘ (2023) â€“ An orchestrator LLM manages a team of micro-agents (each controlling a tool like a coder, tester, etc.) to perform software development tasks end-to-end.â€¨- MAgentic-ONE ã€61â€ ã€‘ (2023) â€“ A generalist Orchestrator agent plans and tracks workflows for specialists like WebSurfer, FileEditor, Coder, achieving strong results on web and API benchmarks[64].â€¨- MAS-GPT ã€338â€ ã€‘ (2023) â€“ Trains a single transformer to output executable code that instantiates a custom multi-agent workflow (agents + their communication pattern) for a given query, essentially one-shot building a multi-agent system.â€¨- MetaAgent ã€372â€ ã€‘ (2023) â€“ Provides a formal finite-state machine template for multi-agent processes (states, transitions, tools) and uses an LLM to fill in the specifics of that FSM for each task.â€¨- AOP (Agent-Oriented Programming) ã€129â€ ã€‘ (2023) â€“ Defines design principles (solvability, completeness, non-redundancy) for orchestrators and demonstrates an orchestrator that decomposes tasks, assigns to agents, and evaluates outcomes with a learned reward model[65].
ğŸŒ Applications & Benchmarks
Real-world domains and evaluation benchmarks where agentic reasoning is applied.
Embodied Agents & Robotics
Agents operating in simulated or physical embodied environments (video games, simulators, real robots). These tests require perception-action loops and long-horizon planning.â€¨- AvalonBench ã€142â€ ã€‘ (2023) â€“ Evaluates LLM agents playing a social deduction game (Avalon) with multiple players, focusing on role reasoning and theory-of-mind[66][67].â€¨- ScienceWorld ã€239â€ ã€‘ (2022) â€“ Tests if agents can perform lab science procedures (mix chemicals, etc.) via textual physics simulators[68].â€¨- CRAFT & Minecraft â€“ Open-ended crafting game tasks (e.g., MineDojo platform) to assess skill learning and tool use in world simulations.â€¨(Embodied benchmarks assess an agentâ€™s ability to handle partial observability, continuous feedback, and goal-driven interaction over long sequences.)
Scientific Discovery
Agents that assist in scientific research, like forming hypotheses, running experiments, or discovering new knowledge.â€¨- The AI Scientist ã€38â€ ã€‘ (2025) â€“ A framework for agents to autonomously propose hypotheses and design experiments (e.g., in chemistry or physics domains).â€¨- Polymath AI â€“ Agents that tackle open math or scientific problems by collaborating (inspired by the Polymath Project for human mathematicians).â€¨- Benchmarks: e.g. evaluating agents on tasks like finding a new synthetic route for a molecule, or discovering a causal gene network from data.
Autonomous Research Agents
Generalist agents aiming to perform open-ended research or exploration tasks without human intervention.â€¨- Agentic Reasoning for Deep Research ã€112â€ ã€‘ (2023) â€“ Proposes an agent that can read literature, form research plans, and iterate experiments in a closed loop for domains like materials science or medicine.â€¨- AutoResearcher â€“ Hypothetical agents that take a broad goal (â€œdiscover a better battery materialâ€) and autonomously generate sub-goals (read papers, run simulations, propose experiments).â€¨- Evaluation: Often involves long-term simulations or case studies since there is no single right answer â€“ e.g., see if the agent finds a known solution or generates publishable insights over days of autonomous work.
Medical & Clinical Agents
Agents specialized for healthcare settings â€“ answering medical questions, monitoring patients, or even suggesting diagnoses/treatments (with human oversight). Reliability and safety are paramount.â€¨- CLIN ã€17â€ ã€‘ (2023) â€“ A continually learning clinical agent that adapts rapidly to new medical tasks and data. Emphasizes generalization and safety in medical Q&A[69].â€¨- Med*Benchmarks: MedMCQA, PubMedQA for medical Q&A; MedBrowser or DoctorGPT for clinical decision support.â€¨- MedBrowseComp ã€247â€ ã€‘ (2025) â€“ Benchmarking agents on using medical search tools and databases for complex medical queries (e.g., finding clinical trial info) â€“ requiring multi-step retrieval and reasoning[70].
Web Navigation & Browsing
Agents that use a web browser to search for information, navigate websites, fill forms, etc. (Often a key subtask for larger goals like travel planning or data collection.)â€¨- WorkArena ã€6â€ ã€‘ (2024) â€“ Measures how well agents can accomplish knowledge work tasks via web browsing (e.g., finding info across multiple pages).â€¨- WebArena / WebShop â€“ Simulated web environment benchmarks where agents must purchase items or find facts using a browser.â€¨- WebAgentBench ã€322â€ ã€‘ (2024) â€“ A suite evaluating multi-turn web navigation under various scenarios (search queries, clicking links, extracting data).
Tool-Use & API Agents
Benchmarks focusing on an agentâ€™s ability to use external APIs or tools correctly.â€¨- ToolQA ã€399â€ ã€‘ (2023) â€“ 1,530 QA dialogs involving 13 different tools (calculator, wiki browser, etc.), testing single-turn tool usage[71].â€¨- APIBench (ã€206â€ ã€‘ 2023) â€“ Large-scale benchmark of 1,645 real-world APIs (Hugging Face, TorchHub, etc.) used to train Gorilla, which assesses generalization to unseen APIs[72].â€¨- ToolBench (ToolLLM) ã€221â€ ã€‘ (2023) â€“ 16,464 API calls across 49 categories; yields ToolLLaMA with strong multi-tool skills[73].â€¨- MetaTool-TOOLE ã€95â€ ã€‘ (2023) â€“ 20,000+ tool usage examples; tests tool selection under similar tools, reliability issues, and multi-tool scenarios[74].â€¨- T-Eval ã€30â€ ã€‘ (2023) â€“ Decomposes the tool use process into sub-skills (instruction following, planning, invocation, etc.) and evaluates each step with ~23k test cases[75].â€¨- GTA (General Tool Agent) ã€291â€ ã€‘ (2023) â€“ 229 challenging tasks using 14 real tools (incl. multi-modal) with real user queries; stresses tool use in realistic settings[76].â€¨- ToolRet ã€254â€ ã€‘ (2023) â€“ Focuses on retrieval augmentation in tool use (a specialized benchmark for tools that retrieve information).
Multi-Agent Collaboration Benchmarks
Evaluating agents on tasks that involve multiple agents and their interaction dynamics.â€¨- DeCrypto ã€7â€ ã€‘ (2025) â€“ A theory-of-mind challenge where agents play a social deduction game (requires reasoning about other agentsâ€™ knowledge)[77].â€¨- BenchMARL ã€15â€ ã€‘ (JMLR 2024) â€“ A general suite for Multi-Agent Reinforcement Learning algorithms, now being used to test LLM-based agents in classic MARL scenarios (like cooperative navigation, predator-prey).â€¨- Crew-Wildfire ã€109â€ ã€‘ (2024) â€“ A multi-agent benchmark for coordinating wildfire response (each agent has different capabilities; tests communication and role assignment in high stakes planning).â€¨- MLAgentBench ã€218â€ ã€‘ (2024) â€“ Evaluating large language model agents on a variety of multi-agent teamwork tasks (from collaborative puzzles to competitive games) to benchmark their emergent coordination abilities.

This list is based on the survey paper. Contributions welcome! For each paper, please cite the original authors and source.
